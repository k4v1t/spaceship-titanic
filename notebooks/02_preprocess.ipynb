{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "32c11706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e3ae4c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"../data\")\n",
    "proc_dir = data_dir / \"processed\"\n",
    "proc_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bbb69eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv(data_dir / \"train.csv\")\n",
    "test_raw = pd.read_csv(data_dir / \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "25367e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "spend_cols = [\"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\n",
    "cat_cols_base = [\"HomePlanet\", \"Destination\", \"Deck\", \"Side\"]\n",
    "bool_cols = [\"CryoSleep\", \"VIP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "22a8f0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spending distribution function helpers\n",
    "\n",
    "def _spend_shares(df: pd.DataFrame):\n",
    "    total = df[spend_cols].sum(axis=1)\n",
    "    shares = (df[spend_cols].div(total.replace(0, np.nan), axis=0)).fillna(0.0) # Calculate % spends taking care of NaNs\n",
    "    shares.columns = [f\"{c}_share\" for c in spend_cols]\n",
    "    return shares, total\n",
    "\n",
    "def _entropy_from_shares(shares:pd.DataFrame, eps=1e-12):\n",
    "    p = shares.clip(lower=0) + eps\n",
    "    ent = -(p * np.log(p)).sum(axis=1)\n",
    "    return (ent / np.log(shares.shape[1])).rename(\"SpendEntropy\") # Normalised\n",
    "\n",
    "def _herfindahl(shares: pd.DataFrame):\n",
    "    return (shares**2).sum(axis=1).rename(\"SpendHHI\")\n",
    "\n",
    "def _gini_from_values(vals:pd.DataFrame, eps=1e-9):\n",
    "    arr = vals[spend_cols].to_numpy()\n",
    "    arr = np.sort(arr, axis=1)\n",
    "    n = arr.shape[1]\n",
    "    cum = np.cumsum(arr, axis=1)\n",
    "    denom = arr.sum(axis=1) + eps\n",
    "    gini = 1 - 2 * (cum.sum(axis=1) / (n * denom))\n",
    "    return pd.Series(gini, index=vals.index, name=\"SpendGini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0de92880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_spend_shape_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    shares, total = _spend_shares(df)\n",
    "    # dominant spend category and share\n",
    "    dominant_idx = df[spend_cols].values.argmax(axis=1)\n",
    "    dominant_cat = pd.Categorical.from_codes(dominant_idx, spend_cols)\n",
    "    # ensure CryoSleep is boolean, but avoid NA -> int errors by filling NA as False for these two flags\n",
    "    cryo = df[\"CryoSleep\"].astype(\"boolean\")\n",
    "    cryo_filled = cryo.fillna(False)\n",
    "    features = pd.concat([\n",
    "        shares,\n",
    "        _entropy_from_shares(shares),\n",
    "        _herfindahl(shares),\n",
    "        _gini_from_values(df),\n",
    "        pd.Series((df[spend_cols] > 0).sum(axis=1), index=df.index, name=\"SpendNonZeroCount\"),\n",
    "        pd.Series((total > 0).astype(int), index=df.index, name=\"AnySpend\"),\n",
    "        pd.Series(((cryo_filled) & (total > 0)).astype(int), index=df.index, name=\"CryoMismatch\"),\n",
    "        pd.Series(((~cryo_filled) & (total == 0)).astype(int), index=df.index, name=\"ZeroSpendButNotCryo\"),\n",
    "        pd.Series(dominant_cat, index=df.index, name=\"DominantSpendCategory\"),\n",
    "        pd.Series(shares.max(axis=1), index=df.index, name=\"DominantSpendShare\"),\n",
    "        pd.Series(df[spend_cols].std(axis=1) / (df[spend_cols].mean(axis=1)+1e-9), index=df.index, name=\"SpendCV\"),\n",
    "    ], axis=1)\n",
    "    return pd.concat([df, features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f24b7bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add group aggregate information\n",
    "\n",
    "def add_group_aggregates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy() \n",
    "    # prerequisites\n",
    "    if \"Group\" not in df.columns:\n",
    "        df[\"Group\"] = df[\"PassengerId\"].astype(str).str.split(\"_\").str[0]\n",
    "    if \"GroupSize\" not in df.columns:\n",
    "        df[\"GroupSize\"] = df.groupby(\"Group\")[\"Group\"].transform(\"size\")\n",
    "    # passenger index and solo/first flags\n",
    "    df[\"PassengerIdx\"] = df[\"PassengerId\"].astype(str).str.split(\"_\").str[1].astype(int)\n",
    "    df[\"IsFirstInGroup\"] = (df[\"PassengerIdx\"] == 1).astype(int)\n",
    "    df[\"IsSolo\"] = (df[\"GroupSize\"] == 1).astype(int)\n",
    "    grp = df.groupby(\"Group\", observed=True)\n",
    "    # spend aggregates\n",
    "    grp_total_spend = grp[spend_cols].sum().sum(axis=1)\n",
    "    df[\"GroupTotalSpend\"] = df[\"Group\"].map(grp_total_spend).astype(float)\n",
    "    df[\"GroupMeanSpend\"] = df[\"GroupTotalSpend\"] / df[\"GroupSize\"].clip(lower=1)\n",
    "    df[\"GroupSpendPerMember\"] = df[\"GroupMeanSpend\"] \n",
    "    # age aggregates\n",
    "    df[\"GroupAgeMean\"] = grp[\"Age\"].transform(\"mean\")\n",
    "    df[\"GroupAgeStd\"] = grp[\"Age\"].transform(\"std\").fillna(0)\n",
    "    # consistency flags\n",
    "    for col in [\"HomePlanet\", \"Destination\", \"Deck\", \"Side\"]:\n",
    "        nunique = grp[col].transform(lambda s: s.nunique(dropna=True))\n",
    "        df[f\"Group{col}NUnique\"] = nunique\n",
    "        df[f\"GroupAllSame{col}\"] = (nunique == 1).astype(int)\n",
    "    # cryo proportion (feature-only)\n",
    "    df[\"GroupCryoShare\"] = grp[\"CryoSleep\"].transform(lambda s: (s==True).mean())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0cc273da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cabin and interactions\n",
    "\n",
    "deck_order = {\"Unknown\":0, \"A\":1, \"B\":2, \"C\":3, \"D\":4, \"E\":5, \"F\":6, \"G\":7, \"T\":8}\n",
    "\n",
    "def add_cabin_extras(df: pd.DataFrame, n_bins=10) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"DeckOrdinal\"] = df[\"Deck\"].map(deck_order).fillna(0).astype(int)\n",
    "    df[\"SideBinary\"] = df[\"Side\"].map({\"P\":0, \"S\":1}).fillna(0).astype(int)\n",
    "    # robust qcut: if too few uniques, fallback to cut\n",
    "    try:\n",
    "        df[\"CabinNumBin\"] = pd.qcut(df[\"CabinNum\"].rank(method=\"first\"), q=n_bins, labels=False, duplicates=\"drop\")\n",
    "    except Exception:\n",
    "        df[\"CabinNumBin\"] = pd.cut(df[\"CabinNum\"], bins=n_bins, labels=False, include_lowest=True)\n",
    "    df[\"CabinNumBin\"] = df[\"CabinNumBin\"].fillna(-1).astype(int)\n",
    "    return df\n",
    "\n",
    "def add_small_interactions(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    # ensure CryoSleep is boolean, but avoid NA -> int errors by filling NA as False for these two flags\n",
    "    cryo = df[\"CryoSleep\"].astype(\"boolean\")\n",
    "    cryo_filled = cryo.fillna(False)      \n",
    "    df[\"Cryo_x_AnySpend\"] = ((cryo_filled) & df[\"AnySpend\"] == 1).astype(int)\n",
    "    df[\"VIP_x_TotalSpend\"] = (df[\"VIP\"].fillna(False).astype(bool).astype(int) * np.log1p(df[\"TotalSpend\"])).astype(float)\n",
    "    # compact one-hot for small-cardinality interactions\n",
    "    df[\"HPxDest\"] = df[\"HomePlanet\"].astype(str) + \"|\" + df[\"Destination\"].astype(str)\n",
    "    hp_dest_dum = pd.get_dummies(df[\"HPxDest\"], prefix=\"HPxDest\", dummy_na=False)\n",
    "    return pd.concat([df.drop(columns=[\"HPxDest\"]), hp_dest_dum], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "19542f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_cabin(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    cab = df[\"Cabin\"].fillna(\"Unknown/9999/U\").str.split(\"/\", expand=True)\n",
    "    cab.columns = [\"Deck\", \"CabinNum\", \"Side\"]\n",
    "    df = df.assign(Deck=cab[\"Deck\"], Side=cab[\"Side\"])\n",
    "    df[\"CabinNum\"] = pd.to_numeric(cab[\"CabinNum\"], errors=\"coerce\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "16080eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_group_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # PassengerId like '0001_01' -> group '0001'\n",
    "    df = df.copy()\n",
    "    if \"Group\" not in df.columns:\n",
    "        df[\"Group\"] = df[\"PassengerId\"].astype(str).str.split(\"_\").str[0]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "336f3afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_spend(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for c in spend_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    df[\"TotalSpend\"] = df[spend_cols].sum(axis=1, skipna=True)\n",
    "    # log1p versions - mitigates the right-skewness inherent in spending metrics\n",
    "    for c in spend_cols + [\"TotalSpend\"]:\n",
    "        df[f\"{c}_log1p\"] = np.log1p(df[c].fillna(0))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a86d1845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_flags(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    cols_to_flag = [\n",
    "        \"HomePlanet\", \"CryoSleep\", \"Destination\", \"Age\", \"VIP\", \n",
    "        \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\",\n",
    "        \"Name\", \"Deck\", \"Side\", \"CabinNum\"\n",
    "    ]\n",
    "    for c in cols_to_flag:\n",
    "        if c in df.columns:\n",
    "            df[f\"{c}_missing\"] = df[c].isna().astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c8b6fa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    # booleans\n",
    "    for c in bool_cols:\n",
    "        if c in out.columns:\n",
    "            out[c] = out[c].astype(\"boolean\")\n",
    "    # target (train only)\n",
    "    if \"Transported\" in out.columns:\n",
    "        out[\"Transported\"] = out[\"Transported\"].astype(int)\n",
    "    # core transforms\n",
    "    out = split_cabin(out)\n",
    "    out = add_group_features(out)\n",
    "    out = engineer_spend(out)\n",
    "    out = add_missing_flags(out)\n",
    "    out = add_spend_shape_features(out)\n",
    "    out = add_group_aggregates(out)\n",
    "    out = add_cabin_extras(out)\n",
    "    out = add_small_interactions(out)    \n",
    "    return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "91d4be90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a single concatenated frame to ensure consistent mapping\n",
    "train = basic_clean(train_raw)\n",
    "test = basic_clean(test_raw)\n",
    "train[\"is_train\"] = 1\n",
    "test[\"is_train\"] = 0\n",
    "both = pd.concat([train, test], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "59170a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GroupSize\n",
    "both[\"GroupSize\"] = both.groupby(\"Group\")[\"Group\"].transform(\"size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "47c13346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For missing age data, use median age (on training data only to avoid leakage)\n",
    "age_median = both.loc[both[\"is_train\"]==1, \"Age\"].median()\n",
    "both[\"Age\"] = both[\"Age\"].fillna(age_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "47afb27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spending: fill NA with zeroes\n",
    "for c in spend_cols:\n",
    "    both[c] = both[c].fillna(0.0)\n",
    "both[\"TotalSpend\"] = both[spend_cols].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "80215434",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot setitem on a Categorical with a new category (Unknown), set the categories first",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[77]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Categorial columns: fill NA with \"Unknown\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cat_cols_base + [\u001b[33m\"\u001b[39m\u001b[33mDeck\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mSide\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     both[c] = \u001b[43mboth\u001b[49m\u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfillna\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mUnknown\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.astype(\u001b[33m\"\u001b[39m\u001b[33mcategory\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/spaceship-titanic/venv/lib/python3.12/site-packages/pandas/core/generic.py:7368\u001b[39m, in \u001b[36mNDFrame.fillna\u001b[39m\u001b[34m(self, value, method, axis, inplace, limit, downcast)\u001b[39m\n\u001b[32m   7361\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   7362\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   7363\u001b[39m             \u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m parameter must be a scalar, dict \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   7364\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mor Series, but you passed a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   7365\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   7366\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m7368\u001b[39m     new_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfillna\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   7369\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdowncast\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdowncast\u001b[49m\n\u001b[32m   7370\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   7372\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, (\u001b[38;5;28mdict\u001b[39m, ABCSeries)):\n\u001b[32m   7373\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m axis == \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/spaceship-titanic/venv/lib/python3.12/site-packages/pandas/core/internals/base.py:186\u001b[39m, in \u001b[36mDataManager.fillna\u001b[39m\u001b[34m(self, value, limit, inplace, downcast)\u001b[39m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Do this validation even if we go through one of the no-op paths\u001b[39;00m\n\u001b[32m    184\u001b[39m     limit = libalgos.validate_limit(\u001b[38;5;28;01mNone\u001b[39;00m, limit=limit)\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_with_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfillna\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m    \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdowncast\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdowncast\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[43m=\u001b[49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m    \u001b[49m\u001b[43malready_warned\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_AlreadyWarned\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/spaceship-titanic/venv/lib/python3.12/site-packages/pandas/core/internals/managers.py:363\u001b[39m, in \u001b[36mBaseBlockManager.apply\u001b[39m\u001b[34m(self, f, align_keys, **kwargs)\u001b[39m\n\u001b[32m    361\u001b[39m         applied = b.apply(f, **kwargs)\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m         applied = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m     result_blocks = extend_blocks(applied, result_blocks)\n\u001b[32m    366\u001b[39m out = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).from_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m.axes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/spaceship-titanic/venv/lib/python3.12/site-packages/pandas/core/internals/blocks.py:2407\u001b[39m, in \u001b[36mExtensionBlock.fillna\u001b[39m\u001b[34m(self, value, limit, inplace, downcast, using_cow, already_warned)\u001b[39m\n\u001b[32m   2404\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   2405\u001b[39m     \u001b[38;5;66;03m# 3rd party EA that has not implemented copy keyword yet\u001b[39;00m\n\u001b[32m   2406\u001b[39m     refs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2407\u001b[39m     new_values = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfillna\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2408\u001b[39m     \u001b[38;5;66;03m# issue the warning *after* retrying, in case the TypeError\u001b[39;00m\n\u001b[32m   2409\u001b[39m     \u001b[38;5;66;03m#  was caused by an invalid fill_value\u001b[39;00m\n\u001b[32m   2410\u001b[39m     warnings.warn(\n\u001b[32m   2411\u001b[39m         \u001b[38;5;66;03m# GH#53278\u001b[39;00m\n\u001b[32m   2412\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mExtensionArray.fillna added a \u001b[39m\u001b[33m'\u001b[39m\u001b[33mcopy\u001b[39m\u001b[33m'\u001b[39m\u001b[33m keyword in pandas \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2418\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m   2419\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/spaceship-titanic/venv/lib/python3.12/site-packages/pandas/core/arrays/_mixins.py:376\u001b[39m, in \u001b[36mNDArrayBackedExtensionArray.fillna\u001b[39m\u001b[34m(self, value, method, limit, copy)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    374\u001b[39m     \u001b[38;5;66;03m# We validate the fill_value even if there is nothing to fill\u001b[39;00m\n\u001b[32m    375\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_setitem_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m copy:\n\u001b[32m    379\u001b[39m         new_values = \u001b[38;5;28mself\u001b[39m[:]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/spaceship-titanic/venv/lib/python3.12/site-packages/pandas/core/arrays/categorical.py:1590\u001b[39m, in \u001b[36mCategorical._validate_setitem_value\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m   1588\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._validate_listlike(value)\n\u001b[32m   1589\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1590\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_scalar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/spaceship-titanic/venv/lib/python3.12/site-packages/pandas/core/arrays/categorical.py:1615\u001b[39m, in \u001b[36mCategorical._validate_scalar\u001b[39m\u001b[34m(self, fill_value)\u001b[39m\n\u001b[32m   1613\u001b[39m     fill_value = \u001b[38;5;28mself\u001b[39m._unbox_scalar(fill_value)\n\u001b[32m   1614\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1615\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   1616\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot setitem on a Categorical with a new \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1617\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcategory (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfill_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m), set the categories first\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1618\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1619\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m fill_value\n",
      "\u001b[31mTypeError\u001b[39m: Cannot setitem on a Categorical with a new category (Unknown), set the categories first"
     ]
    }
   ],
   "source": [
    "# Categorial columns: fill NA with \"Unknown\"\n",
    "for c in cat_cols_base + [\"Deck\", \"Side\"]:\n",
    "    both[c] = both[c].fillna(\"Unknown\").astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1ce231a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A convenience numeric\n",
    "both[\"CabinNum\"] = both[\"CabinNum\"].fillna(both.loc[both[\"is_train\"]==1, \"CabinNum\"].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "727f6604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature sets for naive, basic and enhanced feature spaces\n",
    "\n",
    "# Minimal columns for naive\n",
    "naive_nums = [\"Age\"] + spend_cols\n",
    "naive_bools = bool_cols\n",
    "naive_cats = [\"HomePlanet\", \"Destination\"]\n",
    "\n",
    "# Basic feature space: add cabin split, total spend and missing flags\n",
    "basic_nums = naive_nums + [\"TotalSpend\"] + [\"CabinNum\"]\n",
    "basic_bools = naive_bools\n",
    "basic_cats = cat_cols_base\n",
    "basic_extras = [\"Age_missing\"] + [f\"{c}_missing\" for c in spend_cols] + [\"AnySpend\"]\n",
    "\n",
    "# Enhanced: add group features, logs, ratios and CabinNum\n",
    "enh_nums = basic_nums + [\n",
    "    \"GroupSize\", \"GroupTotalSpend\", \"GroupMeanSpend\", \"GroupMeanSpendPerMember\",\n",
    "    \"GroupAgeMean\", \"GroupAgeStd\", \n",
    "    \"DominantSpendShare\", \"SpendNonZeroCount\", \"SpendEntropy\", \"SpendHHI\", \"SpendGini\", \"SpendCV\",\n",
    "    \"CabinNumBin\", \"DeckOrdinal\", \"SideBinary\",\n",
    "    \"PassengerIdx\", \"IsFirstInGroup\", \"IsSolo\",\n",
    "    \"SpendPerAge\"\n",
    "] + [\"f{c}_log1p\" for c in spend_cols + [\"TotalSpend\"]]\n",
    "enh_bools = basic_bools\n",
    "enh_cats = [\"DominantSpendCategory\"] \n",
    "enh_extras = basic_extras + [\n",
    "    \"CryoMismatch\", \"ZeroSpendButNotCryo\",\n",
    "    \"GroupHomePlanetNUnique\", \"GroupDestinationNUnique\", \"GroupDeckNUnique\", \"GroupSideNUnique\", \n",
    "    \"GroupAllSameHomePlanet\", \"GroupAllSameDestination\", \"GroupAllSameDeck\", \"GroupAllSameSide\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5e668a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolean_to_int(series: pd.Series, fillna_value: int=0) -> pd.Series:\n",
    "    # convert pandas boolean dtype to integers\n",
    "    # True -> 1, False -> 0, NA -> fillna_value\n",
    "    return series.astype(\"boolean\").astype(\"Int8\").fillna(fillna_value).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fc4000f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_set(df: pd.DataFrame, num_cols, bool_cols, cat_cols, extra_cols=(), one_hot=True):\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "    # numerics\n",
    "    for c in num_cols:\n",
    "        if c in df.columns:\n",
    "            out[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0)\n",
    "    # booleans\n",
    "    for c in bool_cols:\n",
    "        if c in df.columns:\n",
    "            out[c] = boolean_to_int(df[c], fillna_value=0)\n",
    "    # extras - usually numerics\n",
    "    for c in extra_cols:\n",
    "        if c in df.columns:\n",
    "            out[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0)\n",
    "    # categoricals\n",
    "    cat_df = pd.DataFrame(index=df.index)\n",
    "    for c in cat_cols:\n",
    "        if c in df.columns:\n",
    "            cat_df[c] = df[c].astype(\"category\")\n",
    "    if one_hot and cat_df.shape[1] > 0:\n",
    "        cat_df = pd.get_dummies(cat_df, dummy_na=False)\n",
    "        out = pd.concat([out, cat_df], axis=1)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3af174e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build all three feature spaces on the concatenated frame (ensures columns are aligned)\n",
    "X_naive_all = build_set(both, naive_nums, naive_bools, naive_cats, extra_cols=[])\n",
    "X_basic_all = build_set(both, basic_nums, basic_bools, basic_cats, extra_cols=basic_extras)\n",
    "X_enh_all = build_set(both, enh_nums, enh_bools, enh_cats, extra_cols=enh_extras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c337a5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-split to train/test\n",
    "mask_tr = both[\"is_train\"] == 1\n",
    "y = train[\"Transported\"].astype(int).values # target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "51feb944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_save(name, X_all):\n",
    "    X_train = X_all.loc[mask_tr].copy()\n",
    "    X_test = X_all.loc[~mask_tr].copy()\n",
    "    # Attach target for train\n",
    "    train_out = X_train.copy()\n",
    "    train_out[\"Transported\"] = y\n",
    "    # Save\n",
    "    train_path = proc_dir / f\"train_{name}.csv\"\n",
    "    test_path = proc_dir / f\"test_{name}.csv\"\n",
    "    train_out.to_csv(train_path, index=False)\n",
    "    X_test.to_csv(test_path, index=False)\n",
    "    print(f\"Save: {train_path.name}, {test_path.name} | shapes: {train_out.shape}, {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "223b0bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save: train_naive.csv, test_naive.csv | shapes: (8693, 15), (4277, 14)\n",
      "Save: train_basic.csv, test_basic.csv | shapes: (8693, 36), (4277, 35)\n",
      "Save: train_enhanced.csv, test_enhanced.csv | shapes: (8693, 50), (4277, 49)\n"
     ]
    }
   ],
   "source": [
    "split_save(\"naive\", X_naive_all)\n",
    "split_save(\"basic\", X_basic_all)\n",
    "split_save(\"enhanced\", X_enh_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5bae51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

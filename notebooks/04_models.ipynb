{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18102909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08cbf83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"../data\")\n",
    "proc_dir = data_dir / \"processed\"\n",
    "raw_test = data_dir / \"test.csv\" # to get the passenger ids\n",
    "sub_dir = Path(\"../submissions\")\n",
    "sub_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d25447d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = [\"naive\", \"basic\", \"enhanced\"]\n",
    "models = [\"logreg\", \"rf\", \"lgbm\"]\n",
    "random_state = 42\n",
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e18db3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features(name: str):\n",
    "    train = pd.read_csv(proc_dir / f\"train_{name}.csv\")\n",
    "    test = pd.read_csv(proc_dir / f\"test_{name}.csv\")\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ceca706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_xy(df: pd.DataFrame):\n",
    "    y = df[\"Transported\"].astype(int).values\n",
    "    X = df.drop(columns=[\"Transported\"])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc3b17ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(name: str):\n",
    "    if name == \"logreg\":\n",
    "        # logistic regression will be scaled inside CV loop\n",
    "        return \"logreg\"\n",
    "    if name == \"rf\":\n",
    "        return RandomForestClassifier(\n",
    "            n_estimators=500,\n",
    "            max_depth=None,\n",
    "            min_samples_split=2,\n",
    "            min_samples_leaf=1,\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    if name == \"lgbm\":\n",
    "        return lgb.LGBMClassifier(\n",
    "            n_estimators=1000,\n",
    "            learning_rate=0.03,\n",
    "            max_depth=-1,\n",
    "            num_leaves=31,\n",
    "            subsample=0.9,\n",
    "            colsample_bytree=0.9,\n",
    "            random_state=random_state,\n",
    "            objective=\"binary\",\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    raise ValueError(f\"Unknown model: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91128cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_score_model(X, y, model_name: str, n_splits=n_splits, seed=random_state):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    accs, aucs = [], []\n",
    "    oof = np.zeros(len(X), dtype=float)\n",
    "    for train_idx, val_idx in skf.split(X, y):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        if model_name == \"logreg\":\n",
    "            scaler = StandardScaler(with_mean=False)\n",
    "            X_train_s = scaler.fit_transform(X_train)\n",
    "            X_val_s = scaler.fit_transform(X_val)\n",
    "            clf = LogisticRegression(max_iter=2000, n_jobs=None, random_state=seed)\n",
    "            clf.fit(X_train_s, y_train)\n",
    "            prob_val = clf.predict_proba(X_val_s)[:, 1]\n",
    "        else:\n",
    "            clf = get_model(model_name)\n",
    "            clf.fit(X_train, y_train)\n",
    "            prob_val = clf.predict_proba(X_val)[:, 1]\n",
    "        oof[val_idx] = prob_val\n",
    "        pred_val = (prob_val >= 0.5).astype(int)\n",
    "        accs.append(accuracy_score(y_val, pred_val))\n",
    "        aucs.append(roc_auc_score(y_val, prob_val))\n",
    "    return {\n",
    "        \"acc_mean\": float(np.mean(accs)),\n",
    "        \"acc_std\": float(np.std(accs)),\n",
    "        \"auc_mean\": float(np.mean(aucs)),\n",
    "        \"auc_std\": float(np.std(aucs)),\n",
    "        \"oof\": oof\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9a64560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_full_and_predict(X, y, X_test, model_name: str):\n",
    "    if model_name == \"logreg\":\n",
    "        scaler = StandardScaler(with_mean=False)\n",
    "        X_full = scaler.fit_transform(X)\n",
    "        Xt = scaler.transform(X_test)\n",
    "        clf = LogisticRegression(max_iter=2000, n_jobs=None, random_state=random_state)\n",
    "        clf.fit(X_full, y)\n",
    "        prob_test = clf.predict_proba(Xt)[:, 1]\n",
    "    else:\n",
    "        clf = get_model(model_name)\n",
    "        clf.fit(X, y)\n",
    "        prob_test = clf.predict_proba(X_test)[:, 1]\n",
    "    return prob_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3809737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Feature set: naive ===\n",
      "logreg | ACC 0.7857 +/- 0.0121 | AUC 0.8706 +/- 0.0065\n",
      "    rf | ACC 0.7850 +/- 0.0079 | AUC 0.8614 +/- 0.0062\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 3502, number of negative: 3452\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011128 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1369\n",
      "[LightGBM] [Info] Number of data points in the train set: 6954, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n",
      "[LightGBM] [Info] Start training from score 0.014380\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 3502, number of negative: 3452\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000796 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1369\n",
      "[LightGBM] [Info] Number of data points in the train set: 6954, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n",
      "[LightGBM] [Info] Start training from score 0.014380\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 3502, number of negative: 3452\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001428 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1368\n",
      "[LightGBM] [Info] Number of data points in the train set: 6954, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n",
      "[LightGBM] [Info] Start training from score 0.014380\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 3503, number of negative: 3452\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000473 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1370\n",
      "[LightGBM] [Info] Number of data points in the train set: 6955, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503666 -> initscore=0.014666\n",
      "[LightGBM] [Info] Start training from score 0.014666\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 3503, number of negative: 3452\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000570 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1369\n",
      "[LightGBM] [Info] Number of data points in the train set: 6955, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503666 -> initscore=0.014666\n",
      "[LightGBM] [Info] Start training from score 0.014666\n",
      "  lgbm | ACC 0.7867 +/- 0.0081 | AUC 0.8681 +/- 0.0052\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 4378, number of negative: 4315\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007893 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1370\n",
      "[LightGBM] [Info] Number of data points in the train set: 8693, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503624 -> initscore=0.014495\n",
      "[LightGBM] [Info] Start training from score 0.014495\n",
      "\n",
      "=== Feature set: basic ===\n",
      "logreg | ACC 0.7899 +/- 0.0071 | AUC 0.8790 +/- 0.0070\n",
      "    rf | ACC 0.8010 +/- 0.0127 | AUC 0.8852 +/- 0.0070\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 3502, number of negative: 3452\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001419 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1915\n",
      "[LightGBM] [Info] Number of data points in the train set: 6954, number of used features: 34\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n",
      "[LightGBM] [Info] Start training from score 0.014380\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 3502, number of negative: 3452\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000792 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1915\n",
      "[LightGBM] [Info] Number of data points in the train set: 6954, number of used features: 34\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n",
      "[LightGBM] [Info] Start training from score 0.014380\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 3502, number of negative: 3452\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000732 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1914\n",
      "[LightGBM] [Info] Number of data points in the train set: 6954, number of used features: 34\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n",
      "[LightGBM] [Info] Start training from score 0.014380\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 3503, number of negative: 3452\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002326 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1916\n",
      "[LightGBM] [Info] Number of data points in the train set: 6955, number of used features: 34\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503666 -> initscore=0.014666\n",
      "[LightGBM] [Info] Start training from score 0.014666\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 3503, number of negative: 3452\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000710 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1915\n",
      "[LightGBM] [Info] Number of data points in the train set: 6955, number of used features: 34\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503666 -> initscore=0.014666\n",
      "[LightGBM] [Info] Start training from score 0.014666\n",
      "  lgbm | ACC 0.8095 +/- 0.0033 | AUC 0.9019 +/- 0.0048\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 4378, number of negative: 4315\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001522 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1916\n",
      "[LightGBM] [Info] Number of data points in the train set: 8693, number of used features: 34\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503624 -> initscore=0.014495\n",
      "[LightGBM] [Info] Start training from score 0.014495\n",
      "\n",
      "=== Feature set: enhanced ===\n",
      "logreg | ACC 0.7921 +/- 0.0085 | AUC 0.8720 +/- 0.0052\n",
      "    rf | ACC 0.7910 +/- 0.0073 | AUC 0.8777 +/- 0.0072\n",
      "[LightGBM] [Info] Number of positive: 3502, number of negative: 3452\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007539 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4215\n",
      "[LightGBM] [Info] Number of data points in the train set: 6954, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n",
      "[LightGBM] [Info] Start training from score 0.014380\n",
      "[LightGBM] [Info] Number of positive: 3502, number of negative: 3452\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005315 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4217\n",
      "[LightGBM] [Info] Number of data points in the train set: 6954, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n",
      "[LightGBM] [Info] Start training from score 0.014380\n",
      "[LightGBM] [Info] Number of positive: 3502, number of negative: 3452\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008393 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4218\n",
      "[LightGBM] [Info] Number of data points in the train set: 6954, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n",
      "[LightGBM] [Info] Start training from score 0.014380\n",
      "[LightGBM] [Info] Number of positive: 3503, number of negative: 3452\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008341 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4214\n",
      "[LightGBM] [Info] Number of data points in the train set: 6955, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503666 -> initscore=0.014666\n",
      "[LightGBM] [Info] Start training from score 0.014666\n",
      "[LightGBM] [Info] Number of positive: 3503, number of negative: 3452\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003536 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4219\n",
      "[LightGBM] [Info] Number of data points in the train set: 6955, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503666 -> initscore=0.014666\n",
      "[LightGBM] [Info] Start training from score 0.014666\n",
      "  lgbm | ACC 0.8021 +/- 0.0043 | AUC 0.8941 +/- 0.0029\n",
      "[LightGBM] [Info] Number of positive: 4378, number of negative: 4315\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007958 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4223\n",
      "[LightGBM] [Info] Number of data points in the train set: 8693, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503624 -> initscore=0.014495\n",
      "[LightGBM] [Info] Start training from score 0.014495\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>featureset</th>\n",
       "      <th>model</th>\n",
       "      <th>acc_mean</th>\n",
       "      <th>auc_mean</th>\n",
       "      <th>submission</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>basic</td>\n",
       "      <td>lgbm</td>\n",
       "      <td>0.809502</td>\n",
       "      <td>0.901853</td>\n",
       "      <td>../submissions/basic_lgbm.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>enhanced</td>\n",
       "      <td>lgbm</td>\n",
       "      <td>0.802140</td>\n",
       "      <td>0.894103</td>\n",
       "      <td>../submissions/enhanced_lgbm.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>basic</td>\n",
       "      <td>rf</td>\n",
       "      <td>0.800989</td>\n",
       "      <td>0.885170</td>\n",
       "      <td>../submissions/basic_rf.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>enhanced</td>\n",
       "      <td>logreg</td>\n",
       "      <td>0.792132</td>\n",
       "      <td>0.871954</td>\n",
       "      <td>../submissions/enhanced_logreg.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>enhanced</td>\n",
       "      <td>rf</td>\n",
       "      <td>0.790980</td>\n",
       "      <td>0.877696</td>\n",
       "      <td>../submissions/enhanced_rf.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basic</td>\n",
       "      <td>logreg</td>\n",
       "      <td>0.789946</td>\n",
       "      <td>0.879032</td>\n",
       "      <td>../submissions/basic_logreg.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>naive</td>\n",
       "      <td>lgbm</td>\n",
       "      <td>0.786723</td>\n",
       "      <td>0.868103</td>\n",
       "      <td>../submissions/naive_lgbm.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>naive</td>\n",
       "      <td>logreg</td>\n",
       "      <td>0.785689</td>\n",
       "      <td>0.870605</td>\n",
       "      <td>../submissions/naive_logreg.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>naive</td>\n",
       "      <td>rf</td>\n",
       "      <td>0.784999</td>\n",
       "      <td>0.861376</td>\n",
       "      <td>../submissions/naive_rf.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  featureset   model  acc_mean  auc_mean                          submission\n",
       "5      basic    lgbm  0.809502  0.901853       ../submissions/basic_lgbm.csv\n",
       "8   enhanced    lgbm  0.802140  0.894103    ../submissions/enhanced_lgbm.csv\n",
       "4      basic      rf  0.800989  0.885170         ../submissions/basic_rf.csv\n",
       "6   enhanced  logreg  0.792132  0.871954  ../submissions/enhanced_logreg.csv\n",
       "7   enhanced      rf  0.790980  0.877696      ../submissions/enhanced_rf.csv\n",
       "3      basic  logreg  0.789946  0.879032     ../submissions/basic_logreg.csv\n",
       "2      naive    lgbm  0.786723  0.868103       ../submissions/naive_lgbm.csv\n",
       "0      naive  logreg  0.785689  0.870605     ../submissions/naive_logreg.csv\n",
       "1      naive      rf  0.784999  0.861376         ../submissions/naive_rf.csv"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_test = pd.read_csv(raw_test) # for passenger id\n",
    "sub_summary = []\n",
    "\n",
    "for fs in feature_sets:\n",
    "    print(f\"\\n=== Feature set: {fs} ===\")\n",
    "    train_df, test_df = load_features(fs)\n",
    "    X, y = split_xy(train_df)\n",
    "    X_test = test_df.copy() # keep a copy of the test feature frame\n",
    "    # evaluate all models via CV\n",
    "    for m in models:\n",
    "        scores = cv_score_model(X, y, m)\n",
    "        print(f\"{m:>6} | ACC {scores[\"acc_mean\"]:.4f} +/- {scores[\"acc_std\"]:.4f} \"\n",
    "              f\"| AUC {scores[\"auc_mean\"]:.4f} +/- {scores[\"auc_std\"]:.4f}\")\n",
    "        # train on full dataset and predict test for submission\n",
    "        prob_test = fit_full_and_predict(X, y, X_test, m)\n",
    "        pred_test = (prob_test >= 0.5)\n",
    "        # submission file\n",
    "        sub = pd.DataFrame({\n",
    "            \"PassengerId\": raw_test[\"PassengerId\"],\n",
    "            \"Transported\": pred_test.astype(bool)\n",
    "        })\n",
    "        out_path = sub_dir / f\"{fs}_{m}.csv\"\n",
    "        sub.to_csv(out_path, index=False)\n",
    "        sub_summary.append({\n",
    "            \"featureset\": fs,\n",
    "            \"model\": m,\n",
    "            \"acc_mean\": scores[\"acc_mean\"],\n",
    "            \"auc_mean\": scores[\"auc_mean\"],\n",
    "            \"submission\": str(out_path)\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(sub_summary).sort_values([\"acc_mean\", \"auc_mean\"], ascending=False)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28745686",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
